"""Data ingestion pipeline with full metadata tagging."""

import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
import uuid
import hashlib
import mimetypes
import os

from .kafka_service import KafkaService
from .opensearch_service import OpenSearchService
from .seaweedfs_service import SeaweedFSService
from .tika_service import TikaService
from .redis_service import RedisService


class IngestionPipeline:
    """Complete data ingestion pipeline with metadata extraction and tagging."""
    
    def __init__(
        self,
        kafka: KafkaService,
        opensearch: OpenSearchService,
        seaweedfs: SeaweedFSService,
        tika: TikaService,
        redis: RedisService
    ):
        self.kafka = kafka
        self.opensearch = opensearch
        self.seaweedfs = seaweedfs
        self.tika = tika
        self.redis = redis
        
        self.stats = {
            "total_ingested": 0,
            "total_bytes": 0,
            "by_type": {},
            "errors": 0
        }
    
    async def ingest_file(
        self,
        file_content: bytes,
        filename: str,
        source: str,
        tags: Optional[List[str]] = None,
        custom_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Ingest a file with full processing pipeline."""
        ingestion_id = str(uuid.uuid4())
        start_time = datetime.utcnow()
        
        # Calculate file hash for deduplication
        file_hash = hashlib.sha256(file_content).hexdigest()
        
        # Check for duplicates
        cached = await self.redis.cache_get(f"file_hash:{file_hash}")
        if cached:
            return {
                "status": "duplicate",
                "existing_id": cached,
                "file_hash": file_hash
            }
        
        # Detect content type
        content_type, _ = mimetypes.guess_type(filename)
        if not content_type:
            content_type = await self.tika.detect_content_type(file_content)
        
        # Extract content and metadata using Tika
        extraction = await self.tika.analyze_document(file_content, content_type)
        
        # Store file in SeaweedFS
        storage_result = await self.seaweedfs.upload_file(
            file_content=file_content,
            filename=filename,
            path=f"/ingested/{start_time.strftime('%Y/%m/%d')}",
            metadata={
                "ingestion_id": ingestion_id,
                "source": source,
                "content_type": content_type
            }
        )
        
        # Build comprehensive metadata
        metadata = {
            "ingestion_id": ingestion_id,
            "filename": filename,
            "file_hash": file_hash,
            "file_size": len(file_content),
            "content_type": content_type,
            "source": source,
            "storage_fid": storage_result.get("fid"),
            "storage_path": storage_result.get("path"),
            
            # Tika extraction
            "extracted_content": extraction.get("content", "")[:50000],  # Limit size
            "detected_language": extraction.get("language"),
            "word_count": extraction.get("statistics", {}).get("word_count", 0),
            "character_count": extraction.get("statistics", {}).get("character_count", 0),
            
            # Document metadata from Tika
            "document_metadata": extraction.get("metadata", {}),
            
            # Custom metadata
            "custom_metadata": custom_metadata or {},
            
            # Tags
            "tags": tags or [],
            
            # Timestamps
            "ingested_at": start_time.isoformat(),
            "processed_at": datetime.utcnow().isoformat()
        }
        
        # Auto-generate tags based on content
        auto_tags = self._generate_auto_tags(metadata, extraction)
        metadata["tags"] = list(set(metadata["tags"] + auto_tags))
        
        # Index in OpenSearch
        doc_id = await self.opensearch.index_document(
            index="documents",
            document=metadata,
            doc_id=ingestion_id
        )
        
        # Generate embeddings for vector search
        if extraction.get("content"):
            try:
                # Chunk content for embedding
                chunks = self._chunk_text(extraction["content"], chunk_size=500)
                for i, chunk in enumerate(chunks[:10]):  # Limit chunks
                    # Note: Embeddings would be generated by the embedding model
                    await self.opensearch.index_document(
                        index="vectors",
                        document={
                            "content": chunk,
                            "doc_id": ingestion_id,
                            "chunk_index": i,
                            "source": source,
                            "metadata": {"filename": filename}
                        }
                    )
            except Exception as e:
                print(f"Embedding error: {e}")
        
        # Cache file hash for deduplication
        await self.redis.cache_set(f"file_hash:{file_hash}", ingestion_id, ttl=86400 * 30)
        
        # Publish to Kafka
        await self.kafka.publish_ingest(
            data_type="file",
            content={"ingestion_id": ingestion_id, "filename": filename},
            source=source,
            tags=metadata["tags"],
            metadata={"content_type": content_type, "size": len(file_content)}
        )
        
        # Update stats
        self.stats["total_ingested"] += 1
        self.stats["total_bytes"] += len(file_content)
        ext = os.path.splitext(filename)[1].lower()
        self.stats["by_type"][ext] = self.stats["by_type"].get(ext, 0) + 1
        
        return {
            "status": "success",
            "ingestion_id": ingestion_id,
            "document_id": doc_id,
            "storage": storage_result,
            "metadata": metadata,
            "processing_time": (datetime.utcnow() - start_time).total_seconds()
        }
    
    async def ingest_data(
        self,
        data: Dict[str, Any],
        data_type: str,
        source: str,
        tags: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Ingest structured data."""
        ingestion_id = str(uuid.uuid4())
        
        document = {
            "ingestion_id": ingestion_id,
            "data_type": data_type,
            "source": source,
            "data": data,
            "tags": tags or [],
            "ingested_at": datetime.utcnow().isoformat()
        }
        
        # Auto-tag based on data type
        auto_tags = [data_type, source]
        document["tags"] = list(set(document["tags"] + auto_tags))
        
        # Index in OpenSearch
        doc_id = await self.opensearch.index_document(
            index="documents",
            document=document,
            doc_id=ingestion_id
        )
        
        # Publish to Kafka
        await self.kafka.publish_ingest(
            data_type=data_type,
            content=data,
            source=source,
            tags=document["tags"]
        )
        
        self.stats["total_ingested"] += 1
        
        return {
            "status": "success",
            "ingestion_id": ingestion_id,
            "document_id": doc_id
        }
    
    async def ingest_event(
        self,
        event_type: str,
        source: str,
        actor: str,
        action: str,
        target: Optional[str] = None,
        data: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Ingest an event."""
        event = {
            "event_type": event_type,
            "source": source,
            "actor": actor,
            "action": action,
            "target": target,
            "data": data or {},
            "tags": tags or [event_type, action],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Index in OpenSearch
        doc_id = await self.opensearch.index_document(
            index="events",
            document=event
        )
        
        # Publish to Kafka
        await self.kafka.publish(
            topic_key="events",
            payload=event,
            message_type="event",
            source=source,
            tags=event["tags"]
        )
        
        return {
            "status": "success",
            "event_id": doc_id,
            "event": event
        }
    
    async def ingest_log(
        self,
        level: str,
        message: str,
        source: str,
        agent: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Ingest a log entry."""
        log_entry = {
            "level": level,
            "message": message,
            "source": source,
            "agent": agent,
            "context": context or {},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Index in OpenSearch
        doc_id = await self.opensearch.index_document(
            index="logs",
            document=log_entry
        )
        
        # Publish to Kafka
        await self.kafka.publish_log(level, message, source, context)
        
        return {
            "status": "success",
            "log_id": doc_id
        }
    
    async def batch_ingest(
        self,
        items: List[Dict[str, Any]],
        source: str
    ) -> Dict[str, Any]:
        """Batch ingest multiple items."""
        results = []
        success_count = 0
        error_count = 0
        
        for item in items:
            try:
                item_type = item.get("type", "data")
                
                if item_type == "file":
                    result = await self.ingest_file(
                        file_content=item["content"],
                        filename=item["filename"],
                        source=source,
                        tags=item.get("tags"),
                        custom_metadata=item.get("metadata")
                    )
                elif item_type == "event":
                    result = await self.ingest_event(
                        event_type=item.get("event_type", "unknown"),
                        source=source,
                        actor=item.get("actor", "system"),
                        action=item.get("action", "unknown"),
                        target=item.get("target"),
                        data=item.get("data"),
                        tags=item.get("tags")
                    )
                else:
                    result = await self.ingest_data(
                        data=item.get("data", item),
                        data_type=item.get("data_type", "unknown"),
                        source=source,
                        tags=item.get("tags")
                    )
                
                results.append(result)
                if result.get("status") == "success":
                    success_count += 1
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                results.append({
                    "status": "error",
                    "error": str(e)
                })
        
        return {
            "total": len(items),
            "success": success_count,
            "errors": error_count,
            "results": results
        }
    
    def _generate_auto_tags(
        self,
        metadata: Dict[str, Any],
        extraction: Dict[str, Any]
    ) -> List[str]:
        """Generate automatic tags based on content."""
        tags = []
        
        # Content type tags
        content_type = metadata.get("content_type", "")
        if "pdf" in content_type:
            tags.append("pdf")
        elif "image" in content_type:
            tags.append("image")
        elif "text" in content_type:
            tags.append("text")
        elif "spreadsheet" in content_type or "excel" in content_type:
            tags.append("spreadsheet")
        
        # Language tag
        language = extraction.get("language", "").strip()
        if language and language != "unknown":
            tags.append(f"lang:{language}")
        
        # Size tags
        size = metadata.get("file_size", 0)
        if size > 10 * 1024 * 1024:
            tags.append("large-file")
        elif size < 1024:
            tags.append("small-file")
        
        # Source tag
        source = metadata.get("source", "")
        if source:
            tags.append(f"source:{source}")
        
        return tags
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """Chunk text for embedding."""
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_chunk.append(word)
            current_size += len(word) + 1
            
            if current_size >= chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_size = 0
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def get_stats(self) -> Dict[str, Any]:
        """Get ingestion statistics."""
        return {
            **self.stats,
            "stats_at": datetime.utcnow().isoformat()
        }
